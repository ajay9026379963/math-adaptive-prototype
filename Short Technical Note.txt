
---

## Short Technical Note (1–2 pages)

### Math Adventures — Technical Note (ML-based Adaptive Engine)

**Purpose**  
Create an adaptive learning prototype for basic arithmetic (ages 5–10) where difficulty adjusts dynamically according to user performance. The prototype demonstrates how a small ML model can personalize challenge level in real time.

**Architecture / Flow**
1. **User input**: learner name and starting difficulty (easy/medium/hard), optional operation choice.
2. **Puzzle generator** (`puzzle_generator.py`) produces a math question aligned to current difficulty.
3. **Attempt tracking** (`tracker.py`) records correctness and response time for each question.
4. **Adaptive engine** (`adaptive_engine.py`):
   - Maintains a dataset of past feature → action pairs.
   - Features are computed over a recent window: accuracy_last_w, avg_time_last_w, streak, last_correct, current_difficulty.
   - For the first N samples (12 by default) it uses a **rule-based heuristic**:
     - If accuracy ≥ 0.8 and avg_time small or streak ≥3 → increase difficulty.
     - If accuracy < 0.5 or repeated errors → decrease difficulty.
     - Else → stay.
   - After accumulating ≥ N labeled examples, a **Decision Tree Classifier** (sklearn) is trained on all collected examples and used to predict the next difficulty. The tree gives interpretability of decisions.
5. **Loop**: after each attempt, the chosen difficulty (heuristic or model prediction) is stored as the label for that feature vector and used for model training later. Session ends after configured rounds; a summary is displayed.

**Adaptive logic rationale**
- Decision tree chosen for:
  - Interpretability (good for educational demos and quick debugging).
  - Works quickly on small datasets.
- The heuristic bootstraps sensible behavior until there's enough session data to train.
- Features chosen are compact and meaningful:
  - Recent accuracy captures proficiency.
  - Avg response time gives speed/fluency signal.
  - Streak and last_correct capture short-term momentum.
  - Current difficulty situates decisions relative to current level.

**Key metrics tracked and influence**
- **Correctness (binary)** — primary signal for competence; directly inflates accuracy and streak.
- **Response time (seconds)** — fast correct answers imply fluency and readiness to progress.
- **Streak** — persistent correct responses indicate readiness to increase challenge.
- **Rolling accuracy** (windowed) — avoids over-reacting to single mistakes.
These metrics are combined in the feature vector input to the model/heuristic. The model maps these to one of three actions: decrease/stay/increase (encoded as difficulty level 0/1/2).

**Why this approach**
- For an internship prototype, combining a simple heuristic with an interpretable ML model gives:
  - Immediate reasonable behavior for new users (heuristic).
  - A path towards personalization as data accumulates (ML).
  - Easy explanation/demo for evaluators (decision tree rules).

**How to collect real data to improve the model**
- Instrument multiple sessions across learners; log anonymized features + context (age bucket, operation, device).
- Aggregate thousands of attempts across many learners to train a more robust model (could consider RandomForest, Gradient Boosting, or small neural nets).
- Use offline evaluation with held-out sessions; measure whether model recommendations keep learners in “optimal challenge zone” (target accuracy, e.g., 70–90%).

**Handling noisy/inconsistent performance**
- Use smoothing and windows (already used).
- Add per-user baselines and use exponential moving averages to handle variability.
- Add confidence thresholds for model predictions; if low confidence revert to heuristic.
- Use outlier filtering for extreme response times (distractors).

**Trade-offs** (Rule-based vs ML)
- Rule-based: simple, deterministic, explainable, good for small data — but less personalized and brittle.
- ML-based: can find patterns beyond handcrafted rules and personalize, but needs data, may overfit, and requires careful evaluation and safety/interpretability measures.
A hybrid design (heuristic bootstrapping + ML when data available) balances the trade-offs — which is what the prototype implements.

**Scaling beyond math**
- Replace the puzzle generator with domain-specific content; reuse same tracking & engine.
- Features may change (e.g., for reading comprehension track hints used, re-reads).
- Consider content metadata (topic, subskill) as features and adapt per-skill.
- Multi-armed bandit or reinforcement learning approaches could be used for better long-term mastery optimization.

---

## Quick demo / sample usage
- Run `python src/main.py --rounds 20`.
- The engine will start heuristic decisions and after ~12 examples start to train and use a decision tree.
- The console shows per-round difficulty, correct/incorrect feedback, and a final summary.

---

If you want, next steps I can do right now (pick any combination below; I will perform it immediately in this reply):

- Package these files into a ZIP and provide a download link.  
- Create a ready-to-paste GitHub repo (I can produce a `git` command sequence and commit message).  
- Convert the console app into a Jupyter notebook or Streamlit web UI (I will implement the UI and code).  
- Add unit tests for components (puzzle generator & adaptive logic).  
- Produce a 60–120s demo script (what to say/record) and sample terminal recording steps.

Which of those would you like me to do next?
